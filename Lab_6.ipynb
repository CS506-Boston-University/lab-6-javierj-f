{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1G3ghc8PaKx_",
   "metadata": {
    "id": "1G3ghc8PaKx_"
   },
   "source": [
    "# Walkthrough: Computing SVD via Eigen-Decomposition\n",
    "\n",
    "Below is a step-by-step explanation of what each part of the code is doing and why it works.\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Goal and Key Idea\n",
    "\n",
    "We want the SVD of a real matrix $A \\in \\mathbb{R}^{m\\times n}:$\n",
    "$$A = U\\,\\Sigma\\,V^\\top,$$\n",
    "where:\n",
    "- $U \\in \\mathbb{R}^{m\\times m}$ has orthonormal columns (left singular vectors),\n",
    "- $V \\in \\mathbb{R}^{n\\times n}$ has orthonormal columns (right singular vectors),\n",
    "- $\\Sigma$ is diagonal with non-negative singular values.\n",
    "\n",
    "\n",
    "## 1) Eigen-decomposition of $A^\\top A$\n",
    "\n",
    "\n",
    "- Form the symmetric, positive semidefinite matrix $A^\\top A \\in \\mathbb{R}^{n\\times n}$.\n",
    "- Compute its eigenvalues (`eigvals`) and eigenvectors (`V`).\n",
    "  - Columns of `V` are eigenvectors.\n",
    "  - For exact arithmetic, all eigenvalues are $\\ge 0$. Numerically, tiny negatives can occur.\n",
    "\n",
    "\n",
    "\n",
    "## 2) Sort eigenpairs by descending eigenvalue\n",
    "\n",
    "- SVD convention orders singular values from largest to smallest.\n",
    "- We reorder eigenvalues and their eigenvectors accordingly.\n",
    "\n",
    "\n",
    "\n",
    "## 3) Singular values = sqrt of eigenvalues (clipped)\n",
    "\n",
    "\n",
    "- Convert eigenvalues to singular values: $\\sigma_i = \\sqrt{\\max(\\lambda_i, 0)}$.\n",
    "- `np.clip` guards against tiny negative values due to floating-point error.\n",
    "\n",
    "\n",
    "\n",
    "## 4) Compute left singular vectors \\(U\\)\n",
    "\n",
    "- Using the relation $u_i = \\frac{A v_i}{\\sigma_i}$ when $\\sigma_i \\neq 0$.\n",
    "- We skip division for effectively zero singular values (rank-deficient directions).\n",
    "\n",
    "\n",
    "## 5) Orthonormalize/normalize columns of \\(U\\)\n",
    "\n",
    "\n",
    "- Normalizes each nonzero column to unit length.\n",
    "- The tiny $`1e-15`$ prevents divide-by-zero.\n",
    "\n",
    "> **Note:** For perfectly computed $U$, normalization would be redundant; numerically it helps keep columns unit-norm.\n",
    "\n",
    "\n",
    "\n",
    "## 7) Return $U$, $\\Sigma$, $V^\\top$\n",
    "\n",
    "- The function returns:\n",
    "  - `U` (left singular vectors, as columns),\n",
    "  - `singular_vals` (the diagonal of $\\Sigma$),\n",
    "  - `V.T` (transpose of right singular vectors).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OPOCvu8xFgb3",
   "metadata": {
    "id": "OPOCvu8xFgb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Installing collected packages: numpy, opencv-python\n",
      "Successfully installed numpy-2.2.6 opencv-python-4.12.0.88\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\17734\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "LD5t03Q5XVLX",
   "metadata": {
    "id": "LD5t03Q5XVLX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U =\n",
      " [[ 0.67121386  0.46028211  0.58104418]\n",
      " [ 0.39435899 -0.88545482  0.24586735]\n",
      " [ 0.62765671  0.06411043 -0.77584593]]\n",
      "Singular values = [4.36673634 3.19226543 0.86084563]\n",
      "V^T =\n",
      " [[ 0.65829392  0.71211175  0.24402042]\n",
      " [ 0.75010116 -0.64777241 -0.13318839]\n",
      " [-0.06322468 -0.27071711  0.96058049]]\n",
      "Reconstruction error: 7.652133485838011e-15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def svd_via_eig(A):\n",
    "    \"\"\"\n",
    "    Compute the Singular Value Decomposition (SVD) of matrix A\n",
    "    using eigen-decomposition: A = U Σ V^T\n",
    "    \"\"\"\n",
    "    # Ensure A is float (important for numerical stability)\n",
    "    A = np.array(A, dtype=float)\n",
    "\n",
    "    # ----------------------------\n",
    "    # TODO Step 1: Compute eigen-decomposition of A^T A (utilize np.linalg.eig to get the eigenvalues and eigenvectors)\n",
    "    # ----------------------------\n",
    "    AtA = A.T @ A\n",
    "    eigvals, V = np.linalg.eig(AtA)\n",
    "    # ----------------------------\n",
    "    # Implementation Ends Here\n",
    "    # ----------------------------\n",
    "\n",
    "    sort_idx = np.argsort(eigvals)[::-1]\n",
    "\n",
    "    # ----------------------------\n",
    "    # TODO Step 2: Sort eigenvalues and eigenvectors in descending order\n",
    "    # Hint: For the eigenvectors, we want to sort by the columns\n",
    "    # ----------------------------\n",
    "    eigvals = eigvals[sort_idx]\n",
    "    V = V[:, sort_idx]\n",
    "    # ----------------------------\n",
    "    # Implementation Ends Here\n",
    "    # ----------------------------\n",
    "\n",
    "\n",
    "    # Step 3: Singular values are sqrt of eigenvalues (non-negative)\n",
    "    singular_vals = np.sqrt(np.clip(eigvals, 0, None))\n",
    "\n",
    "    U = np.zeros((A.shape[0], V.shape[1]))\n",
    "    for i in range(len(singular_vals)):\n",
    "        if singular_vals[i] > 1e-12:\n",
    "    # ----------------------------\n",
    "    # TODO Step 4: Compute U = A V / Σ\n",
    "    # Avoid division by zero for zero singular values\n",
    "    # ----------------------------\n",
    "            U[:, i] = (A @ V[:, i]) / singular_vals[i]\n",
    "    # ----------------------------\n",
    "    # Implementation Ends Here\n",
    "    # ----------------------------\n",
    "\n",
    "    # Step 5: Ensure orthonormality (normalize columns of U)\n",
    "    for i in range(U.shape[1]):\n",
    "        U[:, i] /= np.linalg.norm(U[:, i]) + 1e-15\n",
    "\n",
    "    # Step 6: Return U, Σ, V^T\n",
    "    return U, singular_vals, V.T\n",
    "\n",
    "# Example usage:\n",
    "A = np.array([[3., 1., 1.],\n",
    "              [-1., 3., 1.],\n",
    "              [2., 2., 0.]])\n",
    "\n",
    "U, S, Vt = svd_via_eig(A)\n",
    "\n",
    "print(\"U =\\n\", U)\n",
    "print(\"Singular values =\", S)\n",
    "print(\"V^T =\\n\", Vt)\n",
    "\n",
    "# Verify reconstruction\n",
    "A_recon = U @ np.diag(S) @ Vt\n",
    "print(\"Reconstruction error:\", np.linalg.norm(A - A_recon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "zGOZ-ju9XVLX",
   "metadata": {
    "id": "zGOZ-ju9XVLX"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_image\u001b[39m(image_path):\n\u001b[32m      5\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load image in grayscale\"\"\"\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\"Load image in grayscale\"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return img\n",
    "\n",
    "image_path = \"face.png\"\n",
    "img = load_image(image_path)\n",
    "\n",
    "# Display the original image\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(\"Original Face Image (Walter White aka Heisenberg)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F3M2EnlIXVLX",
   "metadata": {
    "id": "F3M2EnlIXVLX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compress_image_svd(img, k):\n",
    "    \"\"\"Compress an image using SVD by keeping only k singular values\"\"\"\n",
    "    # ----------------------------\n",
    "    # TODO Step 1: Compute full SVD using numpy's SVD\n",
    "    # U: Left Singular Vectors. S: Singular Values. Vt: Right Singular Vectors Transposed.\n",
    "    # Set full_matrices=False for efficiency.\n",
    "    # ----------------------------\n",
    "    U, S, Vt = np.linalg.svd(img, full_matrices=False)\n",
    "    # ----------------------------\n",
    "    # Implementation Ends Here\n",
    "    # ----------------------------\n",
    "\n",
    "    # Step 2: Keep only top-k singular values\n",
    "    S_k = np.zeros((k, k))  # Create a k x k Sigma matrix\n",
    "    np.fill_diagonal(S_k, S[:k])  # Fill diagonal with top-k singular values\n",
    "    U_k = U[:, :k]  # Take first k columns of U\n",
    "    Vt_k = Vt[:k, :]  # Take first k rows of Vt\n",
    "\n",
    "    # TODO Step 3: Reconstruct the image using U dot S dot V^T (use matrix multiplication function in numpy, utilize the shorthand version)\n",
    "    compressed_img = U_k @ S_k @ Vt_k\n",
    "\n",
    "    return np.clip(compressed_img, 0, 255).astype(np.uint8)  # Clip to valid pixel range\n",
    "\n",
    "# ----------------------------\n",
    "# TODO Compress image using SVD utilizing compress_image_svd with k=50\n",
    "# ----------------------------\n",
    "k = 50\n",
    "compressed_img = compress_image_svd(img, k)\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "\n",
    "# Display Results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(\"Original Face Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(compressed_img, cmap=\"gray\")\n",
    "plt.title(f\"Compressed Face Image (k={k})\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MCYvnmhkYLO5",
   "metadata": {
    "id": "MCYvnmhkYLO5"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# TODO Compress image using SVD utilizing compress_image_svd with k=15\n",
    "# ----------------------------\n",
    "k = 15\n",
    "compressed_img = compress_image_svd(img, k)\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "\n",
    "# Display Results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(\"Original Face Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(compressed_img, cmap=\"gray\")\n",
    "plt.title(f\"Compressed Face Image (k={k})\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KZFxNH6oYHdP",
   "metadata": {
    "id": "KZFxNH6oYHdP"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# TODO Compress image using SVD utilizing compress_image_svd with k=10\n",
    "# ----------------------------\n",
    "k = 10\n",
    "compressed_img = compress_image_svd(img, k)\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "\n",
    "# Display Results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(\"Original Face Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(compressed_img, cmap=\"gray\")\n",
    "plt.title(f\"Compressed Face Image (k={k})\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nth3pWUR4C_F",
   "metadata": {
    "id": "Nth3pWUR4C_F"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# TODO Compress image using SVD utilizing compress_image_svd with k=5\n",
    "# ----------------------------\n",
    "k = 5\n",
    "compressed_img = compress_image_svd(img, k)\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "\n",
    "# Display Results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(\"Original Face Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(compressed_img, cmap=\"gray\")\n",
    "plt.title(f\"Compressed Face Image (k={k})\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74c775",
   "metadata": {
    "id": "6e74c775"
   },
   "source": [
    "\n",
    "**Goal:** Compare **TF–IDF → KNN** vs **TF–IDF → TruncatedSVD (LSA) → KNN** for **binary sentiment** classification on IMDB reviews.\n",
    "\n",
    "You will:\n",
    "- Load IMDB reviews with text and labels.\n",
    "- Build a TF–IDF → KNN baseline (sweep `k`, metric, weights).\n",
    "- Add Truncated SVD (LSA) → KNN and choose `k_svd` + `n_neighbors` with validation F1.\n",
    "- Evaluate on a held-out test set and discuss trade-offs.\n",
    "\n",
    "> ✅ **Deliverables**: code, plots, final metrics table, and a short written analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21da6d4a",
   "metadata": {
    "id": "21da6d4a"
   },
   "source": [
    "\n",
    "## 0) Setup\n",
    "\n",
    "- Recommended: Python 3.9+  \n",
    "- Install dependencies if needed.\n",
    "\n",
    "> If `datasets` is not installed, uncomment the first line below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b7042",
   "metadata": {
    "id": "519b7042"
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install numpy pandas scikit-learn matplotlib datasets\n",
    "import os, sys, math, json, random, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f7592",
   "metadata": {
    "id": "6a6f7592"
   },
   "source": [
    "\n",
    "## 1) Load IMDB & Create Splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de6c20",
   "metadata": {
    "id": "b1de6c20"
   },
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDB\n",
    "ds = load_dataset(\"imdb\")\n",
    "train_text = ds[\"train\"][\"text\"]\n",
    "train_label = ds[\"train\"][\"label\"]\n",
    "test_text  = ds[\"test\"][\"text\"]\n",
    "test_label = ds[\"test\"][\"label\"]\n",
    "\n",
    "train_text = list(ds[\"train\"][\"text\"])\n",
    "train_label = list(ds[\"train\"][\"label\"])\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# TODO Create the train/validation train using the train_test_split method from sklearn where  X=train_test, y=train_label, test_size=0.2, random_state=RANDOMSTATE and stratify=train_label\n",
    "# ----------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_text, train_label, test_size=0.2, \n",
    "    random_state=RANDOM_STATE, stratify=train_label)\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "X_test, y_test = test_text, test_label\n",
    "print(f\"Train: {len(X_train)}  |  Val: {len(X_val)}  |  Test: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d25206",
   "metadata": {
    "id": "51d25206"
   },
   "source": [
    "\n",
    "> **Fallback (optional): CSV Loader** – If you can’t use `datasets`, put a CSV with `text` and `label` (0/1) and run this instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "odG0vZxdFWxY",
   "metadata": {
    "id": "odG0vZxdFWxY"
   },
   "outputs": [],
   "source": [
    "print(np.unique(y_train))\n",
    "# Get unique values and their counts\n",
    "unique_values, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Print the unique values and their counts\n",
    "print(\"Unique values:\", unique_values)\n",
    "print(\"Counts:\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9E9MhdY7Os6n",
   "metadata": {
    "id": "9E9MhdY7Os6n"
   },
   "outputs": [],
   "source": [
    "for pos_text,text_value in enumerate(X_train[:5]):\n",
    "  print(text_value[:120],y_train[pos_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea4dcb1",
   "metadata": {
    "id": "eea4dcb1"
   },
   "source": [
    "\n",
    "## 2) Baseline: TF–IDF → KNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w0YRxE8k55Vh",
   "metadata": {
    "id": "w0YRxE8k55Vh"
   },
   "source": [
    "## 📘 What is TF-IDF?\n",
    "\n",
    "**TF-IDF** stands for **Term Frequency – Inverse Document Frequency**.  \n",
    "It is a numerical statistic used to reflect **how important a word is** to a document within a collection (called the *corpus*).\n",
    "\n",
    "Formally, it combines two ideas:\n",
    "\n",
    "$$\\text{TF–IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n",
    "\n",
    "where  \n",
    "- \\(t\\) = term (word)  \n",
    "- \\(d\\) = one document  \n",
    "- the corpus = all documents  \n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "**Term Frequency** measures how often a word appears in a specific document.\n",
    "\n",
    "$$\\text{TF}(t, d) = \\frac{\\text{count of term } t \\text{ in document } d}{\\text{total number of terms in } d}$$\n",
    "\n",
    "TF is higher when the term occurs more frequently in the document.\n",
    "  \n",
    "<br><br><br><br>\n",
    "\n",
    "**Inverse Document Frequency** measures how *rare* or *unique* a word is across the entire corpus.\n",
    "\n",
    "$\\text{IDF}(t) = \\log\\left(\\frac{N}{1 + n_t}\\right)$\n",
    "\n",
    "where  \n",
    "- \\(N\\) = total number of documents  \n",
    "- \\(n_t\\) = number of documents containing the term \\(t\\)\n",
    "\n",
    "Intuitively:\n",
    "- Common words like “the”, “and”, or “is” appear in many documents → **low IDF**\n",
    "- Rare or discriminative words like “thriller”, “blockchain” → **high IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555c832",
   "metadata": {
    "id": "6555c832"
   },
   "outputs": [],
   "source": [
    "k_nn_list_vals = [2,5]\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1,2),\n",
    "    max_df=0.9,\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "def eval_knn(model, name, X_tr, y_tr, X_va, y_va):\n",
    "    # ----------------------------\n",
    "    # TODO use the KNN model and fit it to the X_tr using the y_tr labels\n",
    "    # ----------------------------\n",
    "    t0 = time.time()\n",
    "    model.fit(X_tr, y_tr)\n",
    "    # ----------------------------\n",
    "    # Implementation Ends Here\n",
    "    # ----------------------------\n",
    "\n",
    "    fit_s = time.time() - t0\n",
    "\n",
    "    # ----------------------------\n",
    "    #TODO now use the model to predict on the validation set (X_va) set\n",
    "    # ----------------------------\n",
    "    y_hat = model.predict(X_va)\n",
    "    pred_s = time.time() - t0 - fit_s\n",
    "    # ----------------------------\n",
    "    # Implementation Ends Here\n",
    "    # ----------------------------\n",
    "\n",
    "    # ----------------------------\n",
    "    #TODO calculate the accuracy_score and f1_score using sklearn.metrics\n",
    "    # ----------------------------\n",
    "    acc = accuracy_score(y_va, y_hat)\n",
    "    f1  = f1_score(y_va, y_hat)\n",
    "    # ----------------------------\n",
    "    # Implementation Ends Here\n",
    "    # ----------------------------\n",
    "\n",
    "    print(f\"\\n=== {name} (VAL) ===\")\n",
    "    print(\"Accuracy:\", round(acc,4), \" F1:\", round(f1,4), f\"| fit {fit_s:.2f}s, pred {pred_s:.2f}s\")\n",
    "    print(classification_report(y_va, y_hat, digits=4))\n",
    "    return acc, f1\n",
    "\n",
    "baseline_results = []\n",
    "for metric in [\"cosine\", \"euclidean\"]:\n",
    "    for k in k_nn_list_vals:\n",
    "        for w in [\"uniform\", \"distance\"]:\n",
    "            pipe = Pipeline([\n",
    "                (\"tfidf\", tfidf),\n",
    "                (\"knn\", KNeighborsClassifier(n_neighbors=k, metric=metric, weights=w))\n",
    "            ])\n",
    "            acc, f1 = eval_knn(pipe, f\"TFIDF+KNN | k={k} | {metric} | {w}\",\n",
    "                               X_train, y_train, X_val, y_val)\n",
    "            baseline_results.append(dict(model=\"TFIDF+KNN\", k=k, metric=metric, weights=w, acc=acc, f1=f1))\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_results).sort_values([\"metric\",\"k\",\"weights\"]).reset_index(drop=True)\n",
    "baseline_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd7d4e",
   "metadata": {
    "id": "c9dd7d4e"
   },
   "source": [
    "\n",
    "## 3) Truncated SVD (LSA) → KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafe8c8",
   "metadata": {
    "id": "4dafe8c8"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def make_svd_knn(k_components, n_neighbors, metric=\"cosine\", weights=\"distance\"):\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", tfidf),\n",
    "        # ----------------------------\n",
    "        #TODO use TruncatedSVD where the n_components=k_components, random_state=RANDOM_STATE\n",
    "        # ----------------------------\n",
    "        (\"svd\", TruncatedSVD(n_components=k_components, random_state=RANDOM_STATE)),\n",
    "        # ----------------------------\n",
    "        # Implementation Ends Here\n",
    "        # ----------------------------\n",
    "        (\"norm\", Normalizer(norm=\"l2\")),\n",
    "        (\"knn\", KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, weights=weights))\n",
    "    ])\n",
    "\n",
    "rows = []\n",
    "k_svd_list = [50]\n",
    "\n",
    "for k_svd in k_svd_list:\n",
    "    for k_nn in k_nn_list_vals:\n",
    "        for metric in [\"cosine\", \"euclidean\"]:\n",
    "            for w in [\"uniform\", \"distance\"]:\n",
    "                pipe = make_svd_knn(k_svd, k_nn, metric=metric, weights=w)\n",
    "                pipe.fit(X_train, y_train)\n",
    "                y_val_hat = pipe.predict(X_val)\n",
    "                acc = accuracy_score(y_val, y_val_hat)\n",
    "                f1  = f1_score(y_val, y_val_hat)\n",
    "                evr = pipe.named_steps[\"svd\"].explained_variance_ratio_.sum()\n",
    "                rows.append({\"k_svd\": k_svd, \"k_nn\": k_nn, \"metric\": metric, \"weights\": w,\n",
    "                             \"acc\": acc, \"f1\": f1, \"evr\": evr})\n",
    "\n",
    "svd_df = pd.DataFrame(rows).sort_values([\"k_svd\",\"k_nn\",\"metric\",\"weights\"]).reset_index(drop=True)\n",
    "svd_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608a3c0",
   "metadata": {
    "id": "7608a3c0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plots (matplotlib only; single charts, default colors)\n",
    "sub = svd_df[(svd_df[\"metric\"]==\"cosine\") & (svd_df[\"weights\"]==\"distance\")]\n",
    "plt.figure()\n",
    "for k_nn in sorted(sub[\"k_nn\"].unique()):\n",
    "    s = sub[sub[\"k_nn\"]==k_nn]\n",
    "    plt.plot(s[\"k_svd\"], s[\"f1\"], marker=\"o\", label=f\"n_neighbors={k_nn}\")\n",
    "plt.xlabel(\"SVD components (k_svd)\"); plt.ylabel(\"Validation F1\")\n",
    "plt.title(\"F1 vs SVD dimension (cosine, distance weights)\"); plt.grid(True); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for k_svd in sorted(sub[\"k_svd\"].unique()):\n",
    "    s = sub[sub[\"k_svd\"]==k_svd]\n",
    "    plt.plot(s[\"k_nn\"], s[\"f1\"], marker=\"o\", label=f\"k_svd={k_svd}\")\n",
    "plt.xlabel(\"n_neighbors (k_nn)\"); plt.ylabel(\"Validation F1\")\n",
    "plt.title(\"F1 vs neighbors (cosine, distance weights)\"); plt.grid(True); plt.legend(); plt.show()\n",
    "\n",
    "evr_by_k = sub.groupby(\"k_svd\")[\"evr\"].mean()\n",
    "plt.figure()\n",
    "plt.plot(evr_by_k.index, evr_by_k.values, marker=\"o\")\n",
    "plt.xlabel(\"SVD components\"); plt.ylabel(\"Cumulative explained variance ratio\")\n",
    "plt.title(\"Explained variance vs SVD dimension\"); plt.grid(True); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec673e99",
   "metadata": {
    "id": "ec673e99"
   },
   "source": [
    "\n",
    "## 4) Final Selection & **Test** Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450b5e5",
   "metadata": {
    "id": "a450b5e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "best = svd_df.sort_values(\"f1\", ascending=False).iloc[0]\n",
    "best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e83690",
   "metadata": {
    "id": "a3e83690"
   },
   "outputs": [],
   "source": [
    "X_tr_final = np.concatenate([X_train])\n",
    "y_tr_final = np.concatenate([y_train])\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "final_pipe = make_svd_knn(\n",
    "    k_components=int(best[\"k_svd\"]),\n",
    "    n_neighbors=int(best[\"k_nn\"]),\n",
    "    metric=str(best[\"metric\"]),\n",
    "    weights=str(best[\"weights\"])\n",
    ")\n",
    "final_pipe.fit(X_tr_final, y_tr_final)\n",
    "y_test_hat = final_pipe.predict(X_test)\n",
    "\n",
    "print(\"\\n=== FINAL TEST RESULTS (SVD+KNN) ===\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_test_hat), 4))\n",
    "print(\"F1:\", round(f1_score(y_test, y_test_hat), 4))\n",
    "print(classification_report(y_test, y_test_hat, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_test_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4cdd15",
   "metadata": {
    "id": "de4cdd15"
   },
   "source": [
    "\n",
    "## 5) Compare against **TF–IDF-only KNN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee2509",
   "metadata": {
    "id": "77ee2509"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Pick best TFIDF baseline from baseline_df\n",
    "best_base = baseline_df.sort_values(\"f1\", ascending=False).iloc[0]\n",
    "best_base\n",
    "\n",
    "BEST_BASE_METRIC  = best_base[\"metric\"]\n",
    "BEST_BASE_K       = int(best_base[\"k\"])\n",
    "BEST_BASE_WEIGHTS = best_base[\"weights\"]\n",
    "\n",
    "tfidf_only_best = Pipeline([\n",
    "    (\"tfidf\", tfidf),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=BEST_BASE_K, metric=BEST_BASE_METRIC, weights=BEST_BASE_WEIGHTS))\n",
    "])\n",
    "\n",
    "tfidf_only_best.fit(X_tr_final, y_tr_final)\n",
    "y_test_hat_base = tfidf_only_best.predict(X_test)\n",
    "\n",
    "print(f\"\\n=== FINAL TEST RESULTS (TFIDF+KNN: k={BEST_BASE_K}, {BEST_BASE_METRIC}, {BEST_BASE_WEIGHTS}) ===\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_test_hat_base), 4))\n",
    "print(\"F1:\", round(f1_score(y_test, y_test_hat_base), 4))\n",
    "print(classification_report(y_test, y_test_hat_base, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_test_hat_base))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600fe771",
   "metadata": {
    "id": "600fe771"
   },
   "source": [
    "\n",
    "### ✅ Add this comparison table to your report\n",
    "\n",
    "| Features           | Model | SVD k | kNN k | Metric   | Weights   | Test Acc | Test F1 | Notes                    |\n",
    "|--------------------|-------|------:|------:|----------|-----------|---------:|--------:|--------------------------|\n",
    "| TF–IDF             | KNN   |   —   |   7   | cosine   | distance  |    …     |    …    | sparse, high-dim         |\n",
    "| TF–IDF → **SVD**   | KNN   |  200  |   7   | cosine   | distance  |    …     |    …    | dense, low-rank semantic |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df621150",
   "metadata": {
    "id": "df621150"
   },
   "source": [
    "\n",
    "## 6) - Misclassification analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X7TfuJT5PsB8",
   "metadata": {
    "id": "X7TfuJT5PsB8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6afba",
   "metadata": {
    "id": "87b6afba"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Show a few misclassified examples for error analysis\n",
    "\n",
    "def show_examples(X_raw, y_true, y_pred, want=5, which=\"fp\"):\n",
    "    idxs = []\n",
    "    all_correct_tp = []\n",
    "    all_correct_tn = []\n",
    "    for i, (yt, yp) in enumerate(zip(y_true, y_pred)):\n",
    "        if which == \"fp\" and yt == 0 and yp == 1:\n",
    "            idxs.append(i)\n",
    "        if which == \"fn\" and yt == 1 and yp == 0:\n",
    "            idxs.append(i)\n",
    "        if yt == 1 and yp == 1:\n",
    "          all_correct_tp.append(i)\n",
    "        elif yt == 0 and yp == 0:\n",
    "          all_correct_tn.append(i)\n",
    "    idxs = idxs[:want]\n",
    "    print(len(all_correct_tp))\n",
    "    print(len(all_correct_tn))\n",
    "    print(f\"\\nShowing {which.upper()} ({len(idxs)}):\")\n",
    "    for i in idxs:\n",
    "        print(f\"\\n--- idx={i}  true={y_true[i]}  pred={y_pred[i]} ---\")\n",
    "        #print(X_raw[i][:500].replace(\\\"\\\\n\\\",\\\" \\\"))\n",
    "        print(X_raw[i][:500])\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: Use the SVD + KNN model we defined and predict on our validation set\n",
    "#       show_examples will take the validation set with the corresponding X_val and y_val\n",
    "#       along with your predictions\n",
    "# ----------------------------\n",
    "y_val_hat_best = final_pipe.predict(X_val)\n",
    "show_examples(X_val, y_val, y_val_hat_best, want=1, which=\"fp\")\n",
    "show_examples(X_val, y_val, y_val_hat_best, want=1, which=\"fn\")\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75sJcTtVPxZB",
   "metadata": {
    "id": "75sJcTtVPxZB"
   },
   "outputs": [],
   "source": [
    "print(y_val_hat_best[:10])\n",
    "print(y_val[:10])\n",
    "print(len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VdeJFuIxSt_H",
   "metadata": {
    "id": "VdeJFuIxSt_H"
   },
   "source": [
    "# The next part of the lab will involve submitting to a Kaggle competition in prepartion for the mid-term\n",
    "\n",
    "You can find the Kaggle competition at https://www.kaggle.com/competitions/cs-506-practice-compeition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P2RvBjgdTWBU",
   "metadata": {
    "id": "P2RvBjgdTWBU"
   },
   "source": [
    "1) First you will download the test_df.csv from the Kaggle compeition\n",
    "2) You will use your best trained model to perform predictions on the test_df.csv\n",
    "3) Submit the my_predictions.csv to the Kaggle competition\n",
    "4) The train_df.csv on the Kaggle compeition is the same as the training data we used in this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gk7KTnbNTelN",
   "metadata": {
    "id": "gk7KTnbNTelN"
   },
   "outputs": [],
   "source": [
    "text_values = []\n",
    "numbers = []\n",
    "with open('test_df.csv') as f:\n",
    "  for pos_line,line in enumerate(f):\n",
    "    if pos_line == 0:\n",
    "      continue\n",
    "    text_ = line.strip().split(',', 1)[1]\n",
    "    nums = line.strip().split(',', 1)[0]\n",
    "    #print(nums)\n",
    "    text_values.append(text_)\n",
    "    numbers.append(nums)\n",
    "    #if pos_line == 1000:\n",
    "    #  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hZdGMlNPThI4",
   "metadata": {
    "id": "hZdGMlNPThI4"
   },
   "outputs": [],
   "source": [
    "y_test_hat = final_pipe.predict(text_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8U1n-m8LTjJi",
   "metadata": {
    "id": "8U1n-m8LTjJi"
   },
   "outputs": [],
   "source": [
    "data = {'id': numbers,'label': y_test_hat}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('my_prediction.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
